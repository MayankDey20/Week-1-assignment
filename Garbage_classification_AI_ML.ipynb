{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pzjpSunDbx4F86IUjH_2wsGOWXu2T4qN",
      "authorship_tag": "ABX9TyPMfyr81BenvXtIT4kBdWew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayankDey20/Week-1-assignment/blob/main/Garbage_classification_AI_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruW_XQxohVLC"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Importing NumPy for numerical operations and array manipulations\n",
        "import matplotlib.pyplot as plt  # Importing Matplotlib for plotting graphs and visualizations\n",
        "import seaborn as sns  # Importing Seaborn for statistical data visualization, built on top of Matplotlib\n",
        "import tensorflow as tf  # Importing TensorFlow for building and training machine learning models\n",
        "from tensorflow import keras  # Importing Keras, a high-level API for TensorFlow, to simplify model building\n",
        "from tensorflow.keras import Layer  # Importing Layer class for creating custom layers in Keras\n",
        "from tensorflow.keras.models import Sequential  # Importing Sequential model for building neural networks layer-by-layer\n",
        "from tensorflow.keras.layers import Rescaling , GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks  # Importing various modules for layers, optimizers, and callbacks in Keras\n",
        "from sklearn.utils.class_weight import compute_class_weight  # Importing function to compute class weights for imbalanced datasets\n",
        "from tensorflow.keras.applications import EfficientNetV2B2  # Importing EfficientNetV2S model for transfer learning\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Importing functions to evaluate model performance\n",
        "import gradio as gr  # Importing Gradio for creating interactive web interfaces for machine learning models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "EtjQYje1rxVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/drive/My Drive/TrashType_Image_Dataset\"\n",
        "image_size = (124, 124)\n",
        "batch_size = 32\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "KK0HkPU0sDQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    shuffle=True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    shuffle=True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "440F6U2AtWkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of batches in the validation dataset\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "\n",
        "# Split the validation dataset into two equal parts:\n",
        "# First half becomes the test dataset\n",
        "test_ds = val_ds.take(val_batches // 2)\n",
        "\n",
        "# Second half remains as the validation dataset\n",
        "val_dat = val_ds.skip(val_batches // 2)\n",
        "\n",
        "# Optimize test dataset by caching and prefetching to improve performance\n",
        "test_ds_eval = test_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "9b30qP4Qtf3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ds.class_names)\n",
        "print(val_ds.class_names)\n",
        "print(len(train_ds.class_names))\n"
      ],
      "metadata": {
        "id": "dJJAqkDwtjGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(12):\n",
        "    ax = plt.subplot(4, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(train_ds.class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "m9dzwhret0Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_distribution(dataset, class_names):\n",
        "    total = 0\n",
        "    counts = {name: 0 for name in class_names}\n",
        "\n",
        "    for _, labels in dataset:\n",
        "        for label in labels.numpy():\n",
        "            class_name = class_names[label]\n",
        "            counts[class_name] += 1\n",
        "            total += 1\n",
        "\n",
        "    for k in counts:\n",
        "        counts[k] = round((counts[k] / total) * 100, 2)  # Convert to percentage\n",
        "    return counts"
      ],
      "metadata": {
        "id": "FKSJcIDst6by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_bar_plot(dist, title):\n",
        "    plt.bar(dist.keys(), dist.values(), color='cornflowerblue')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1nCrgbWouEpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "\n",
        "# Get class distributions\n",
        "train_dist = count_distribution(train_ds, class_names)\n",
        "val_dist = count_distribution(val_ds, class_names)\n",
        "test_dist = count_distribution(test_ds, class_names)\n",
        "overall_dist = {}\n",
        "for k in class_names:\n",
        "    overall_dist[k] = round((train_dist[k] + val_dist[k]) / 2, 2)\n",
        "\n",
        "print(train_dist)\n",
        "print(val_dist)\n",
        "print(test_dist)\n",
        "print(overall_dist)"
      ],
      "metadata": {
        "id": "efuVmdpKuMlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}